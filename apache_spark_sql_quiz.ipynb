{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# creating a spark session\n",
    "spark = SparkSession.builder.appName(\"Data wrangling with Spark SQL\").getOrCreate()\n",
    "\n",
    "# setting the directory for the data\n",
    "data_dir = 'C:/Users/John/PycharmProjects/customer-attrition/data/'\n",
    "\n",
    "# Reading in the JSON file as a spark dataframe\n",
    "df = spark.read.json(data_dir + 'sparkify_log_small.json')\n",
    "\n",
    "df.createOrReplaceTempView('df_table')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+\n",
      "|page|            page|\n",
      "+----+----------------+\n",
      "|null|Submit Downgrade|\n",
      "|null|       Downgrade|\n",
      "|null|          Logout|\n",
      "|null|   Save Settings|\n",
      "|null|        Settings|\n",
      "|null|        NextSong|\n",
      "|null|         Upgrade|\n",
      "|null|           Error|\n",
      "|null|  Submit Upgrade|\n",
      "+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SELECT distinct pages for the blank user and distinct pages for all users\n",
    "# Right join the results to find pages that blank visitor did not visit\n",
    "\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM (SELECT DISTINCT page\n",
    "    FROM df_table\n",
    "    WHERE userId = \"\") AS blank_pages\n",
    "    RIGHT JOIN (SELECT DISTINCT page\n",
    "    FROM df_table) AS all_pages\n",
    "    ON blank_pages.page = all_pages.page\n",
    "    WHERE blank_pages.page IS NULL\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|       Downgrade|\n",
      "|           Error|\n",
      "|          Logout|\n",
      "|        NextSong|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|Submit Downgrade|\n",
      "|  Submit Upgrade|\n",
      "|         Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Which page did user id \"\"(empty string) NOT visit?\n",
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM (SELECT DISTINCT T1.page\n",
    "    FROM df_table as T1\n",
    "    EXCEPT\n",
    "    SELECT DISTINCT T2.page\n",
    "    FROM df_table as T2 \n",
    "    WHERE T2.userID = \"\") AS T3\n",
    "    ORDER BY T3.page ASC\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     462|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many female users do we have in the data set?\n",
    "spark.sql('''\n",
    "    SELECT COUNT(*)\n",
    "    FROM (SELECT DISTINCT T.userId, T.gender\n",
    "    FROM df_table as T\n",
    "    WHERE T.gender = 'F')\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   462|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is a much better and easy way.\n",
    "spark.sql('''\n",
    "    SELECT COUNT(DISTINCT userId)\n",
    "    FROM df_table\n",
    "    WHERE gender == 'F'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(song)|\n",
      "+-----------+\n",
      "|         83|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How many songs were played from the most played artist?\n",
    "spark.sql('''\n",
    "    SELECT COUNT(T1.song)\n",
    "    FROM df_table as T1\n",
    "    WHERE T1.artist = (SELECT T2.artist\n",
    "    FROM df_table as T2\n",
    "    WHERE T2.artist != \"\"\n",
    "    GROUP BY T2.artist\n",
    "    ORDER BY count(1) DESC\n",
    "    LIMIT 1)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  artist|plays|\n",
      "+--------+-----+\n",
      "|Coldplay|   83|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT artist, COUNT(artist) as plays\n",
    "    FROM df_table\n",
    "    GROUP BY artist\n",
    "    ORDER BY plays DESC\n",
    "    LIMIT 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts = spark.sql('''\n",
    "        SELECT Artist, COUNT(Artist) AS plays\n",
    "        FROM df_table\n",
    "        GROUP BY Artist\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts.createOrReplaceTempView('artist_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Artist|plays|\n",
      "+--------------------+-----+\n",
      "|      The Black Keys|   40|\n",
      "|        STRATOVARIUS|    1|\n",
      "|      The Chameleons|    1|\n",
      "|Dashboard Confess...|    3|\n",
      "|      Jarabe De Palo|    3|\n",
      "|        Ziggy Marley|    1|\n",
      "|        Yann Tiersen|   10|\n",
      "|  The Watts Prophets|    1|\n",
      "|            Goldfish|    1|\n",
      "|           Kate Nash|    3|\n",
      "|              DJ Taz|    1|\n",
      "|    Jane's Addiction|    1|\n",
      "|         Eva Cassidy|    4|\n",
      "|               Rufio|    1|\n",
      "|           Los Lobos|    4|\n",
      "|         Silverstein|    1|\n",
      "|        Rhett Miller|    1|\n",
      "|              Nebula|    1|\n",
      "|Yonder Mountain S...|    1|\n",
      "|        Generation X|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Artist|plays|\n",
      "+--------+-----+\n",
      "|Coldplay|   83|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT a2.Artist, a2.plays FROM \\\n",
    "          (SELECT max(plays) AS max_plays FROM artist_counts) AS a1 \\\n",
    "          JOIN artist_counts AS a2 \\\n",
    "          ON a1.max_plays = a2.plays \\\n",
    "          \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Artist|plays|\n",
      "+--------+-----+\n",
      "|Coldplay|   83|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT A2.Artist, A2.plays\n",
    "    FROM (SELECT MAX(plays) AS max_plays\n",
    "    FROM artist_counts) AS A1\n",
    "    LEFT JOIN artist_counts AS A2\n",
    "    ON A1.max_plays = A2.plays\n",
    "''').show()\n",
    "# we are using the same thing twice just by taking the max from one of them and using left join. Cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|  Artist|plays|\n",
      "+--------+-----+\n",
      "|Coldplay|   83|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here is another solution\n",
    "spark.sql('''\n",
    "        SELECT Artist, COUNT(Artist) AS plays\n",
    "        FROM df_table\n",
    "        GROUP BY Artist\n",
    "        ORDER BY plays DESC\n",
    "        LIMIT 1\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT song)|\n",
      "+--------------------+\n",
      "|                  24|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT COUNT(DISTINCT T.song)\n",
    "    FROM df_table as T\n",
    "    WHERE T.artist = (SELECT artist\n",
    "    FROM df_table\n",
    "    WHERE artist != \"\"\n",
    "    GROUP BY artist\n",
    "    ORDER BY count(1) DESC\n",
    "    LIMIT 1)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_visit = udf(lambda x: 1 if x == 'Home' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "window_val = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.filter((df.page == 'NextSong') | (df.page == 'Home')).select('userId', 'page', 'ts')\\\n",
    ".withColumn('homevisit', home_visit('page')).withColumn('period', Fsum('homevisit').over(window_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count(period))|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp.filter(df_temp.page == 'NextSong').groupBy('userId','period').agg({'period':'count'})\\\n",
    ".agg({'count(period)':'avg'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('home_visit', lambda x: 1 if x == 'Home' else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "window_val = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets tackle the fifth question\n",
    "# How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer.\n",
    "\n",
    "is_home = spark.sql('''\n",
    "    SELECT userId, page, ts, CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home\n",
    "    FROM df_table\n",
    "    WHERE page = 'NextSong' or page = 'Home'\n",
    "''')\n",
    "\n",
    "is_home.createOrReplaceTempView('is_home_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_sum = spark.sql('''\n",
    "    SELECT *, SUM(is_home) OVER (PARTITION BY userId ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS period\n",
    "    FROM is_home_table\n",
    "''')\n",
    "\n",
    "cumulative_sum.createOrReplaceTempView('period_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(count_results)|\n",
      "+------------------+\n",
      "| 6.898347107438017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT AVG(T.count_results)\n",
    "    FROM (SELECT userId, page, period, COUNT(*) as count_results\n",
    "    FROM period_table\n",
    "    GROUP BY userId, page, period\n",
    "    HAVING page = 'NextSong') as T\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+-------+------+\n",
      "|userId|    page|           ts|is_home|period|\n",
      "+------+--------+-------------+-------+------+\n",
      "|  1436|NextSong|1513783259284|      0|     0|\n",
      "|  1436|NextSong|1513782858284|      0|     0|\n",
      "|  2088|    Home|1513805972284|      1|     1|\n",
      "|  2088|NextSong|1513805859284|      0|     1|\n",
      "|  2088|NextSong|1513805494284|      0|     1|\n",
      "|  2088|NextSong|1513805065284|      0|     1|\n",
      "|  2088|NextSong|1513804786284|      0|     1|\n",
      "|  2088|NextSong|1513804555284|      0|     1|\n",
      "|  2088|NextSong|1513804196284|      0|     1|\n",
      "|  2088|NextSong|1513803967284|      0|     1|\n",
      "|  2088|NextSong|1513803820284|      0|     1|\n",
      "|  2088|NextSong|1513803651284|      0|     1|\n",
      "|  2088|NextSong|1513803413284|      0|     1|\n",
      "|  2088|NextSong|1513803254284|      0|     1|\n",
      "|  2088|NextSong|1513803057284|      0|     1|\n",
      "|  2088|NextSong|1513802824284|      0|     1|\n",
      "|  2162|NextSong|1513781246284|      0|     0|\n",
      "|  2162|NextSong|1513781065284|      0|     0|\n",
      "|  2162|NextSong|1513780860284|      0|     0|\n",
      "|  2162|NextSong|1513780569284|      0|     0|\n",
      "+------+--------+-------------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT *\n",
    "    FROM period_table\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
